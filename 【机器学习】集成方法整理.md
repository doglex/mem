---
date: 2017-05-15 21:21
status: public
title: 【机器学习】集成方法整理
---

> 集成方法：多个弱分类器组合成强分类器。
> 集成的前提是弱分类器不至于太差。

1.Voting
2.Stacking(输出做特征，或者输出添加至特征列表)

#I.Bagging
1.本身是降低方差variance

2.易并行
###一、Bagging
1.进行T次BootStrap随机采样(假设训练数据大小m，有放回采m次，得到Dm，约有三分之一的D-Dm可以用做out-of-bag),得到T个弱分类器。

2.对于分类目标，对分类器结果进行投票(多数表决，可以设置阈值)。对于回归目标，可以进行算术平均。
> 1/3= (1-1/m)^m (m->无穷)

###二、随机森林(RF,Random Forest)
```
1.对Bagging的改进（只有这两个改进）
(1)使用CART树作为基分类器（这个和GBRT是一样的）
(2)使用部分特征(列采样)。特征数量越少，越健壮，即variance变小，bias变大。#具体的设置需要根据交叉验证
2.RF的优点：
(1)可并行(适合大数据处理)，实现简单,每棵树独立训练
(2)样本维度高的时候，可以高效训练
(3)可以给出各个特征的权重，即特征重要性(特征选择的方法)
(4)模型方差小，OOB进行无偏估计
(5)对部分缺失值不敏感，对不平衡数据效果也好
(6)一般任务准确度高，可分类可回归
2.RF的缺点：
(1)噪声过大时过拟合
(2)取值划分比较多的特征容易对RF的决策产生更大的影响（具有更大的权重），从而影响拟合的模型的效果
```

#II.Boosting提升方法
1.本身是降低偏差bias
2.不易并行，但是有(GBRT的实现XGBoost和微软的lightGBM等是并行的，XGB在特征粒度进行并行，XGB的总结在我的另一篇博客)
3.逐棵树训练
###一、Adaboost
1.算法步骤：
(1)初始化每个样本权值分布1/N（其中N为样本数）
![](~/4.gif)
(2)对于第m个基分类器，根据带权重的Dm进行训练得到Gm分类器。
> 如何构建带权重的基分类器？比如一个二分类，寻找分裂点，使得加权的MSE最小。

![](~/5.gif)
> 为什么是+1和-1而不是1和0，因为后面误差更新的时候要乘上yi。因此对于二分类问题要注意转化。
> 另外注意这里是分类器而不是回归器。

(3)计算该模型上的误差
**是带权值的0-1误差，或者说是出错的概率，实际上整个Adaboost是指数形式误差。**
![](~/7.gif)
(4)计算这个基分类器的系数（自然对数，反错误率）
> 这个系数技巧对于带权重voting或许很有用

![](~/8.gif)
出错越少，alpha应该是越大的（说明这个基分类器越好），越有投票权，但是分母为0或者分子为0的极端情况应该设计。
同时保证了这个系数是个正值（假设出错不超过一半），若出错超过一半，是负值。
(5)更新训练数据集的权值分布：
![](~/233333.gif)
其中Z是归一化因子，使得每个D都是概率分布，而上两步计算出错的概率时也得到了统一。在这个权重基础上训练下一个基分类器。
这样做的话，
![](~/gogo.gif)
相当于误分类样本的权值得到放大倍数为，
![](~/3.gif)
因此，误分类的样本在下一轮在起更大作用。
(6)训练好M个基分类器之后,进行带权重的投票。误分类越小的分类器，投票权重越大。所有alpha之和不是1.
![](~/111.gif)
2.可以认为模型是多个基分类器的加法模型，损失函数是指数函数，前向分步算法。也可以用于回归任务。
![](~/2222.gif)
3.**Adaboost的训练误差是以指数速率下降的（适应性提升）。**

###二、GBM(GBRT、GBDT、XGBoost、Boosting Trees)提升树
1.不同的提升树算法区别主要在损失函数，回归问题有平方误差损失，分类问题有指数损失，一般决策问题有一般的损失函数。
2.加法模型，第m步的基模型是：
![](~/t1.gif)
相当于是利用观察值y和当前模型的残差去训练新树T，
![](~/t2.gif)
3.以平方损失为例，训练theta参数是为了去拟合残差，或者说是对残差进行训练。![](~/t3.gif)

4.回归问题的提升树算法
利用残差。
(1)初始化f0=0
(2)对每个样本计算残差（残差向量）
(3)拟合残差得到新回归树（比如决策树桩，二划分，每个划分里求MSE最优回归点，遍历划分值），更新fm
(4)最终的fM就是结果。

5.梯度提升GBM
除了平方损失和指数损失，还有一般的损失，不方便直接利用残差。那就对目标的**一阶梯度进行拟合，来对残差进行近似**。（为何可以这样近似，参考XGBoost里的内容，因为有泰勒展开）
（1）定义损失函数L(y,f(x))
（2）初始化
![](~/t4.gif)
(3)对每个样本计算损失的梯度，
![](~/t6.gif)
（4）对梯度（向量，长度为样本量）拟合一个回归树，得到第m棵树的J个叶节点区域：

![](~/t7.gif)
(8)更新模型为，
![](~/t8.gif)
(9)最后的模型为取m=M棵树。

6.XGBoost是GBM的高效实现。

#III.模型比较
###一、GBDT和RF
1.相同点：
(1)都是由多棵树组成
(2)最终结果都是多个树一起决定
(3)都是集成方法
(4)都不容易过拟合（强大数定理）
(5)都可以用于分类、回归

2.不同点：
(1)一个是bagging，一个是boosting
(2)组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
(3)组成随机森林的树可以并行生成；而GBDT只能是串行生成
(4)对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
(5)随机森林对异常值不敏感，GBDT对异常值(outlier)非常敏感
(6)随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
(7)随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能

###二、Adaboost和GBRT
1.相同点：
(1)都是boosting，都是逐棵树训练
(2)都是弱模型的组合
(3)都可以用于分类、回归
2.不同：
(1)Adaboost利用的是提高误分类样本的权重，提升树利用的是观察值与当前预测值的残差
(2)GBRT使用残差是利用一阶梯度进行近似，而XGBoost是利用一二阶梯度

###三、GBRT和XGBoost
> 转知乎wepon
```
1.传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
2.传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。二阶导加速训练。
3.xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
4.Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。学习率防止过拟合。
5.（补充：传统GBDT的实现也有学习速率）列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。列采样防止过拟合。
6.可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
```

#IV.其他问题
1.为什么可以有特征权重？为什么可以用于特征选择？
(1)RF(OOB扰动法)
使用每棵树的OOB数据，给出预测误差e11,e21,e31,..，平均得到e1。对OOB数据的各个维度上加上随机噪声，给出预测误差e12，e22，e23,...,平均得到e2.比较各个特征上的e2-e1.值越大，说明这个特征越重要，经受不起扰动。
(2)GBRT(Gini指数扰动法)
记录各个分裂时候的Gini增益。对每个特征上加入随机扰动，Gini增益变化越大说明这个特征越重要。




