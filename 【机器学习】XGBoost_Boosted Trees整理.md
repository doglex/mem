---
date: 2017-05-14 22:00
status: public
title: 'XGBoost:Boosted Trees整理'
---

XGBoost是一种GBM的实现，是GBDT的高效实现，竞赛大部分用这个获胜，或者用这个结合深度网络
>参考1：《Introduction to Boosted Trees》Tianqi Chen，2014
>参考2：《XGBoost：A Scalable Tree Boosting System》 TianqiChen，Carlos Guestrin，2016
>参考3：https://www.zhihu.com/question/41354392/answer/98658997

# 流程：
- 定义Obj
- 每轮tree最小化Obj
- 最小化残差（二阶泰勒计算值）
- 步长贪婪：最大化分裂增益（求和，枚举特征、分裂点）
- 剪枝

#I.监督学习的关键概念
##一、什么是监督学习
1.训练集中$x_{i} \epsilon \mathbb{R}^{d}$是第i个训练样本。
2.模型(Model)：就是根据$x_{i}$来预测$\hat{y_{i}}$:
(1)线性模型为$\hat{y_{i}}=\sum_{j}\omega_{j}x_{ij}$(包括线性模型/Logistic回归)
(2)监督值$\hat{y_{i}}$可以是线性回归的预测分值(score),可以是Logistic回归(分类任务，$\frac{1}{1+exp(-\hat{y_{i}})}$)中的正类的概率值，也可以是排名任务(Ranking)中的排名。
3.参数(Parameters):就是我们要学习出来的结果。
对线性模型而言$\Theta=\\{w_{j}|j=1,...,d\\}$ //d是参数维度，要注意数据小二维度灾难，数据cover不上的问题，那样要先降维
##二、目标函数（Objective Function）
1.目标函数无处不在
$$Obj(\Theta)=L(\Theta)+\Omega(\Theta)$$
- 第一项是损失函数，表示对训练数据的拟合效果，越小拟合越好。
- 第二项罚项(正则项)，表示模型的复杂程度。

2.损失函数：对训练集的拟合好坏
(1)平方损失(square loss)
$$l(y_{i},\hat{y_{i}})=(y_{i}-\hat{y_{i}})^{2}$$
- MSE(均方误差)，RMSE（均方误差开根）
- MAE(平均绝对差)
- LSM(最小二乘法拟合，即OLS)

(2)对数损失(Logistic loss)
$$l(y_{i},\hat{y_{i}}) = y_{i}ln(1+exp(-\hat{y_{i}}))+(1-y_{i})ln(1+exp(\hat{y_{i}}))$$

3.正则项：模型复杂程度
(1)L2正则： $\Omega(\omega) = \lambda\parallel\omega\parallel^2$，有平滑性，减低所有参数。
(2)L1正则：$\Omega(\omega) = \lambda\parallel\omega\parallel_{1}$，少数特征起作用，有稀疏性，可以用于特征选择。因为在参数的分布范围与正方形比圆形更容易在0处相切。业内称Lasso，也有group lasso，就是对一组写一个式子，容易使整组参数变0。
(3)L0正则：非零参数个数，求解困难，用L1替代。

##三、模型
1.岭回归：线性模型，平方损失，L2正则。求导取0可得闭式解。
$\sum_{i=1}^{n}(y_{i}-\omega ^Tx_{i})+\lambda\parallel\omega\parallel^2$
2.Lasso: 线性模型，平方损失，L1正则。
$\sum_{i=1}^{n}(y_{i}-\omega ^Tx_{i})+\lambda\parallel\omega\parallel_{1}$
3.ElasticNet，结合了L1和L2正则。
4.逻辑斯蒂回归(分类任务，广义线性)：线性模型，对数损失，L2正则。
(1)标准的对数损失为
$L(Y,P(Y|X))=-logP(Y|X)$,对正例而言在X下最大化P(Y)就能最小化损失（最大似然）。注意有负号，取对数时可以使分母上来。
因为二类概率为（求和为1），
![](~/CodeCogsEqn.gif)
代入标准式有（比较的是预测概率值与实际值的差距，用乘法），

![](~/2.gif)
因此有目标式，
![](~/3.gif)
（2）也可以从最大似然推导过来
假设伯努利(独立同二项分布，m次观察)，
有似然式，
$Likelihood=\prod_{i=1}^{m}h_{\theta}(x_{i})^{y_{i}}\cdot (1-h_{\theta}(x_{i}))^{1-y_{i}}$
取对数似然，可以得到相同的式子。而对其取负得到了目标式。
(3)对目标式梯度下降或者拟牛顿法可以进行优化。
二阶：牛顿法 x=x-f'(x)/f''(x)，共轭梯度法，BFGS，L-BFGS
一阶：
下降法 x=x-uf'(x)，Adam，Adagrad，Adadelta，RMSProP，SGD(加Nesterov动量)
随机梯度下降法SGD是mini-batch，引入随机性容易跳出局部点。
(4)为什么是广义线性：log(P/1-P)=y=wx，是线性的
(5)为什么要用LR，因为他是广义线性，对sigmoid形式的数据划分比较好，而线性模型不行。sigmoid函数中间敏感，两边梯度小，在神经网络BP的时候容易梯度弥散(改用ReLu，非线性，弥散小，易求梯度)，而且能量不均衡（相对于tanh的正负1）。
##四、目标函数为何为两项（偏差-方差做折衷）
1.偏差-方差分解
泛化误差=偏差(bias)+方差(variance)+噪声(noise)
2.损失函数有助于拟合上训练数据（降偏差）
3.罚项有助于使模型变得简单且稳定(抗数据干扰，降低方差)
4.然而，RF、Adaboost等模型随着树的数量增长(模型变复杂)，却不容易过拟合(overfitting,在训练集上表现好，在测试集上表现不好),是因为有强大数定理。

#II.回归树（Regression Tree）与集成
##一、回归树
1.和决策树一样特征分裂剪枝，只是叶节点上不上分类标签而是分值(score)，集成就是说把这些分值相加给出结果。
![](~/16-12-53.jpg)
2.使用的一般是CART树
使用Gini指数增益选择最佳分裂，都是衡量不确定性的变化(树生成的过程是去噪过程)，是对信息熵的近似，但计算量小很多。

![](~/4.gif)
意义是任取两个事件类别不一致的概率，也是类别一致的对立事件。
3.其他决策树算法
信息熵为$Ent(D)=-\sum plog(p)$
ID3：信息增益$Gain_{v}=Ent(D)-\sum \frac{D_{v}}{D}Ent(D_{v})$表示按这个特征分裂前后的确定性变化
ID3有一个问题，对于像id那列取值全部不一样的的优先得到了分裂，为了修复这个问题，引入了C4.5：信息增益比$Gain_{r}=\frac{Gain_{v}}{IV_{a}},IV_{a}=-\sum \frac{D_{v}}{D}log\frac{D_{v}}{D}$
##二、集成
1.集成方法有GBM（也GBRT、GBDT，本身XGBoost就是GBM的一种实现），随机森林RF
2.树的优点：特征无需归一化，因为下次输入时可以使用相同的尺度（scale）。
3.集成的优点：可以发现特征间的高阶联系，使用的数据规模可大可小(scalable)，工业界广泛使用。
##三、我们的模型和参数
1.假设有K棵回归树，每棵树用一个函数f将多维数据map成一个score，那么模型就是这些树的和。其中，F表示回归树的空间。
$y_{i}=\sum_{k=1}^{K}f_{k}(x_{i}),f_{k}\in F$
2.f的具体含义是“树的结构（structure）”+各个叶子节点上的值(score).
3.或者我们就说参数就是各个f
$\Theta=\\{{f_{1},f_{2},...f_{K}}\\}$
我们的目标就是去学习到合适的f，而不必去学习具体的参数。
4.那么，我们如何学习得到f，仍然是定义目标函数（损失函数+正则化），之后去优化目标函数。
##四、请问如何学习一棵单一特征的树？
1.考虑我在各个时间上是否喜欢音乐
![](~/16-48-30.jpg)
![](~/16-49-06.jpg)
那么划分点就是一些时间事件，而高度值就是各个叶节点上的score。
2.可以用阶梯函数来抽象这棵回归树，那么两个问题：几个划分点够用（复杂度定义），如何拟合比较好（损失函数）的score。
##五：现在我们可以定义集成树了
1.模型：假设为K棵树的和
$y_{i}=\sum_{k=1}^{K}f_{k}(x_{i}),f_{k}\in F$
2.目标函数：
$Obj(\Theta)=\sum_{i=1}^{n}loss(y_{i},\hat{y_{i}})+\sum_{k=1}^{K}\Omega(f_{k})$
其中损失函数可以是平方损失，可以是对数损失，等等。
复杂度可以是树的深度，节点数，叶节点score的L2、L1，等等。
##六、目标函数与启发式方法(Heuristic)
> 我认为启发式方法是没有办法的办法，无法确保性能、结果。

1.启发式训练树的步骤：利用信息增益(包括熵增益、Gini等)来选择特征进行分裂。进行剪枝或者达到最大深度。然后对叶节点做平滑。
2.信息增益是为了降低拟合时的训练误差，剪枝是一种正则化减少由结构定义的复杂度，最大深度限制了函数空间，平滑叶子值是为了降低L2正则。
3.回归树不仅仅用于回归问题，也可以用于分类、排名。训练只跟目标函数有关，GBRT是平方损失(Adaboost指数损失，SVM合页损失)，LogitBoost是对数损失。

##七、小结
1.偏差-方差分解无处不在
2.回归树学习也是利用了目标函数（损失函数+正则项）
3.因为我们既想要模型简单稳定（剃须刀原则，降方差），又想要对训练数据的拟合效果好（降偏差）。

#III.梯度提升：我们是这样学习的
##一、学习任务
1.目标函数为 $Obj(\Theta)=\sum_{i=1}^{n}loss(y_{i},\hat{y_{i}})+\sum_{k=1}^{K}\Omega(f_{k})$
2.我们不能用传统的下降法（比如SGD）来学习这些f，因为这些是回归树（阶梯函数），而不是数值向量。
3.但是我们可以用增量训练(Boosting,即Additive Traing)。从一个常量树开始，每次增加一个函数（一棵树）。进行不断逼近，不断重新估计y。
![](~/19-09-25.jpg)
4.那么问题来了，每次增加的f应该如何计算？
##二、增量学习(Boosting)
1.若考虑平方损失
![](~/5.gif)
推导是容易的，只需要优化（可以遍历，等方法来做）最后一个式子就可以了。
2.考虑其他损失（更一般的一阶二阶梯度问题）
把$f_{t}$视为增量，那就是目标函数里的损失函数部分的增量，有泰勒展开式(在x附近)，
$f(x+\Delta x)\simeq f(x)+{f}'(x)\Delta x+\frac{1}{2}{f}''(x)\Delta^2 x$
从而有$Obj^{(t)}\simeq \sum_{i=1}^{n}[g_{i}f_{t}(xi)+\frac{1}{2}h_{i}f_{t}^2(x_{i})]+\Omega(f_{t})+const$这就是我们要优化的式子。
其中，
$g_{i}=\frac{d}{d(y^{(t-1)})}loss(y_{i},y^{(t-1)})\\\h_{i}=\frac{d^2}{d(y^{(t-1)})}loss(y_{i},y^{(t-1)}$
对于平方损失，有$g_{i}=2(y^{(t-1)}-y_{i}),h_{i}=2$
> 我觉得既然是泰勒展开，一阶、三阶亦可。

3.要优化的式子为$Obj^{(t)}\simeq \sum_{i=1}^{n}[g_{i}f_{t}(xi)+\frac{1}{2}h_{i}f_{t}^2(x_{i})]+\Omega(f_{t})$
4.结论：现在我们知道了可以优化，而gi和hi来自损失函数，而树（函数）的学习依赖于gi和hi，因此不同的损失函数计算不同的gi和hi即可。

##三、重新定义树与目标函数
1.树：是一堆带索引的叶子，叶子上有值,共有T个叶子。是一个映射函数，把不同样本映射的不同的T个叶子上。
$f_{t}(x)=\omega_{q}(x),\omega\in\mathbb{R}^T,q:\mathbb{R}^d\rightarrow \\{1,2,...,T\\}$
2.复杂度定义
$\Omega(f_{t})=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^{T}\omega_{j}^2$
第一项表示叶子个数，第二项是叶子分值的L2正则。因为XGB的灵活性，不限于此定义。
> 我有个问题，复杂度的定义一个T类，只跟模型有关，是否复杂度可以引入数据分布有关。模型复不复杂也许可以是跟具体的问题的一个相对概念吧。

3.按各个叶子来计算目标函数
![](~/6.gif)
由此，目标函数是T的独立的凸函数之和。
4.优化各个凸函数
求一阶导易得，
$argmin_{x}Gx+\frac{1}{2}Hx^2,H>0\Rightarrow min:-\frac{1}{2}\frac{G^2}{H},at:x=-\frac{G}{H}$
因此，原目标可以改为
![](~/7.gif)
其中，G和H都各个叶子上分配的所有样本的一阶梯度和，二阶梯度和。
从而，估计出最优，
$\omega^*=\frac{G_j}{H_j+\lambda},Obj=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\lambda}+\gamma T$
##四、用暴力搜索新的树不可行
1.暴力枚举树的结构
2.估计目标和叶节点的权重(score)，找到最好的划分
3.结构是无穷的，故不可行
##五、改为贪婪法来一步步生成树
贪婪法是局部优化，不是全局的，非贪婪全局
1.从深度为0开始
2.对每个节点，尝试选择最好分裂：
$Gain=\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}-\gamma$
分裂效果与不分裂效果对比，得到增益，增益越大越能较小损失，越需要分裂。注意增加了一个叶子就增加了一个$\gamma$.
3.还剩一个问题，如何找到最优分割？
##六、找最优分割
1.对各个特征（维度）上先对每个样本排好序。
2.对每个特征尝试线性搜索二分割，找到这个特征上的最优分割，并给出增益。
3.搜索所有的特征，找出最大的增益Gain，进行分裂。
4.每一层的复杂度是O(nlogn),总复杂度是O(nKdlogn).
5.这种搜索复杂度是可以优化的，通过缓存或者近似等方式。同时适用于数据规模的增大。

##七、离散型变量
可以通过one-hot分成多个维度进行。这个搜索算法适用于稀疏的(很多0)，也适用于有缺失值(0)的数据。

##八、剪枝和正则化
1.增益Gain完全可能是负的
2.预剪枝(pre-stopping)：遇到最大增益是负的增益就停止分裂，这样可能损失一些全局上的可能性。
3.后剪枝(post-prunning):先分裂到最大深度，回头递归地删除负增益的叶子。开销大，比较全局优。

##九、整理
1.每次添加一棵树（利用梯度）
2.在每次开始时，计算各个样本上的一阶导、二阶导
$g_{i}=\frac{d}{d(y^{(t-1)})}loss(y_{i},y^{(t-1)})\\\h_{i}=\frac{d^2}{d(y^{(t-1)})}loss(y_{i},y^{(t-1)}$
3.使用这些导数值去贪婪的生成一个树：遍历特征，遍历分裂
$Gain=\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}-\gamma$
4.添加新树到森林，但是不是完全添加，有一个学习率shrinkage：
$y^{(t)}=y{(t-1)}+\epsilon f_t$
有步长的添加保留了后续添加的可能性，一定程度上防止过拟合。

#IV.小结
1.如果不同样本有不同权重怎么办？
设计损失函数带系数让各个样本损失不同。
2.一维划分是否只能贪婪线性搜索？
动态规划或许可以加速。
3.模型+策略(目标)+参数
4.偏差-方差分解是无处不在的，目标函数也是。

#V.几个特别的地方
1.适用于各种规模是数据 Scalable
2.能够处理稀疏数据
3.能够处理数据权重
4.并行分布式，缓存(cache-aware)计算，很快
5.out-of-core非内存计算
6.是个完整的系统，提供各个语言的接口，可移植，支持Spark
7.有学习率shrinkage
8.贪婪搜索各个特征上的分裂
9.对列预先排序(column block)，可以存至不同机器，加速计算
10.适用于分类也适用于回归

#VI.XGB相对于传统GBRT的优点
> 转知乎wepon回答

1.传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
2.传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。二阶导加速训练。
3.xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
4.Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。学习率防止过拟合。
5.（补充：传统GBDT的实现也有学习速率）列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。列采样防止过拟合。
6.可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。