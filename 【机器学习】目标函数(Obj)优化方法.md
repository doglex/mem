---
date: 2017-05-15 19:54
status: public
title: 【机器学习】目标函数(Obj)优化方法
---

目标：minf(x)
#I.梯度法
##一、梯度下降法(gradient descent, steepest descent)
1.迭代方式
![](~/1.gif)
其中lambda是步长，
2.迭代终止条件：函数值之差小于精度或者x变化小于精度。
3.当**目标为凸函数时,梯度下降解是全局最优。**一般情况，是局部最优。
4.收敛速度未必很快的。
##二、牛顿法（Newton method）
1.迭代方式
泰勒二阶展开，求导为0的地方为极小值出。H是海森矩阵，正定时保证是极小值。
![](~/2.gif)
2.比梯度下降法迭代少，更快。为什么，反过来如果不是更好，那何苦去求二阶导？能够利用二阶信息，找到更快的下降。
##三、拟牛顿法
1.为什么要拟牛顿法，因为H矩阵的逆计算复杂，考虑一个矩阵G来替代。
2.拟牛顿法有DFP、BFGS、L-BFGS等。Broyden类算法。
##四、SGD、BGD、MBGD
1.SGD：每次根据一个样本更新（online）。
- 训练速度快
- 不是全局最优，准确度下降

2.BGD：每次根据整体数据的平均进行更新。
- 得到全局最优解
- 样本多的时候会非常慢

3.MBGD：以mini-batch分批，神经网络经常最优做。
- 介于两者之间
- 引入随机性也不容易陷入局部优

##五、下降器的分类
1.一阶下降器：SGD(含有Nesterov动量)、Adam、Adagrad、AdaDelta、RMSProP等。
2.二阶下降器：牛顿、共轭梯度、BFGS、L-BFGS等。


#II.坐标上升法和坐标下降法
1.迭代方式：每次固定其他维度，在一个维度上求极值。然后切换维度。
2.这样的话不是沿着梯度（最快方向），而是沿着坐标轴。

#III.跳出局部最优（对于启发式算法）
1.引入一定随机性（但是不要太随机，至少要有方向性）
2.模拟退火：下去一点，有一定可能上升回来一点。

#IV.其他问题
1.为什么这些主流的下降器一定要通过一步步地下降来达到目的地，而我们爬山的时候只要向下一看就知道哪里是最低了，有没有可能算法直接就知道哪里是最优，像OLS那样直接有闭式解而不需要启发式搜索？
2.有一种引入随机的方式是混合多种相差较大的下降器，因为这个下降器掉进局部最优了，另一个下降器完全可能跳出来，或者同一类下降器因为步长不同而跳出来？另外，是否可以通过一种合理的下降器ensemble方式来减小局部最优的可能性？我尝试用LeNet训练手写数字的时候发现混用下降器效果是变差的。

3.对一个正数s开平方(leetcode上的题)？
- 解法一：进行线性搜索，进行更小粒度的线性搜索。
- 解法二：将线性搜索降为二分搜索。
- 解法三：使用下降器，一阶或者二阶。
- 解法四：使用 r = (r+s/r)/2 进行迭代：保持面积不变，慢慢压到正方形，如果r太小，增大r，r太大就减小r。设置r的初始值为s，那样就处于一边了。
