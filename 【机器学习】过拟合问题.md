---
date: 2017-05-01 17:40
status: public
title: 【机器学习】过拟合问题
---

/## 过拟合现象
- 训练集上效果比测试集上好很多

## 过拟合的原因
- 模型太复杂
- 数据噪声太大
- 归根结底是数据数量不够（深度学习的话就需要更多数据了）
- 维度灾难（curse of dimensionality），导致数据cover不上特征


##　过拟合解决
- 增大数据。
- 参数最大最小限定，在tensorflow做神经网络里有这一项。
- 深度学习中dropout随机删除一部分神经元，引入了随机性，那样发现效果很好。感觉跟Random Forest 的思想有点像，因为RF有列采样（对每个树随机选取一部分特征）。还有pooling可以引入随机性减少参数，还有早停，还有mini-batch的调节。引入随机性，既有利于减小过拟合，也有利于跳出局部最优。
- 罚项：L0 是非零参数个数，求解是NP难问题。
- L1 是差绝对值的平均，比L2有更大稀疏性，称Lasso。解释性也更好。
- L2 是差平方的平均，平滑性。
- 决策树里预剪枝（训练时使用一部分测试数据，来判断是否继续分裂），减少了很多分裂机会，但是快。后剪枝，训练后再剪枝，分裂机会多，更全局，但是慢。
- shrinkage，设置学习步长。
- CNN比全连接网络共享了权值参数，神经元数量大大减少，模型变简单了，需要的数据也少了。可以认为是一种正则化方法。
- 特征选择、降维。实质是减低模型复杂度。

## 欠拟合
- 模型太简单。LR容易欠拟合。