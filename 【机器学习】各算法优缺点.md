---
date: 2017-05-01 17:07
status: public
title: 【机器学习】各算法优缺点
---


## 一、朴素贝叶斯
- 优点：小规模表现好，适合分类任务，适合增量训练。
- 缺点：对输入形式敏感（数值型可以考虑二分？），很多时候要拉普拉斯平滑。

## 二、决策树
- 优点：计算简单，可解释型强，能处理缺失值，不相关特征，能多分类。可以设计为回归模型。
- 缺点：过拟合问题（预剪枝、后剪枝）。RF可以解决一些，因为对数据样本的bagging加上对特征的随机选择（比如选 n**0.5个特征）引入了随机，减小了过拟合。

## 三、logistic
- 优点：实现简单，速度快，存储小。
- 缺点：容易欠拟合，只能二分类（改进，1v1,1v多，多v多），对outlier比较敏感，不支持非线性划分。

## 四、线性拟合（LSM/Ridge/Lasso）
- 优点：实现简单，计算简单（close form，闭式解）。
- 缺点：不能拟合非线性数据。

## 五、kNN
- 优点：思路简单（近朱者赤），理论成熟，可分类可回归，可非线性，对outlier不敏感。
- 缺点：计算量大，样本不平衡，大内存开销。
- 注：其实是免训练的模型。也算优点吧。

## 六、SVM
- 优点：可用于线性/非线性(核：高斯核、多项式核、字符串核、RBF、Logistic核)，可以用于回归SVR，低泛化误差，易解释性，抗数据扰动（outlier），存储小（支持向量）。
- 缺点：需要手动选参数，选核函数，原始的SVM是二分类。

## 七、Adaboost
- 优点：低泛化误差，易实现，准确率高，参数少，多分类。降低偏差bias（预测与实际的差）。
- 缺点：对outlier敏感（因为会调误分类权重），不能并行。
- 注：和GBM（GBRT/GBDT）都是boosting，不同在于权重调整一个根据误分类数据，一个根据梯度。

## 八、Random Forest
- 优点：处理高维数据不用做特征选择；能给出特征重要比；oob，使用无偏估计，泛化能力强；树之间独立，可并行，因而训练快；不平衡数据可以平衡误差；一部分特征遗失仍然可以有精确度。不用剪枝。减低方差variance（数据扰动产生的不同结果）。
- 缺点：在某些噪音较大的分类或者回归问题上过拟合；对于不同取值的属性的数据，取值划分较多的属性会对RF产生更大的影响，导致权值不可信。

## 九、xgboost
- 基分类器除CART外还支持线性分类器，相当于LR、Ridge。
- GBDT用一阶，xgboost用一阶和二阶进行梯度优化。
- 代价函数里有L1和L2正则，控制复杂度减小过拟合。
- shrinkage可以设置学习速率。
- 借鉴了RF的列抽样（选一些特征），引入随机（深度学习的dropout也是引入了随机），降低过拟合，减少计算。
- 缺失值的样本也可以自动学习分裂方向。
- 支持并行，不是tree粒度的，而是特征粒度的。
- 可并行的近似直方图算法，高效生成候选的分割点。

## 十、k-means（属于EM思想）
- 优点：简单快速，团状效果好，O(nkt),通常局部收敛
- 缺点：需要指定k，初始值敏感，需要指定距离定义/中心定义，数据形状敏感(比如密度形状的用DBSCAN，比如流形学习)，对outlier敏感。