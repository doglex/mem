---
date: 2017-06-12 11:02
status: public
title: 【机器学习】机器学习二十五个问题
---

>问题及答案整理自网络，有一些自己表述。
>没有银弹。

##一.LR（Logistic Regression）
1.对数损失函数：
![](~/1.gif)
2.写出LR模型的求解方法：
代入对数损失（等价于求伯努利的极大似然），进行下降法。

3.虽然叫逻辑回归，但是适用于二分类问题。

##二、L1和L2正则
```
1.L0是非零系数的个数，L1是绝对值之和，L2是系数平方和
2.L0具有稀疏性，但是不好求解。用L1进行近似。L1(Lasso)具有稀疏性，L2(Ridge)具有平滑性，混合使用的有ElasticNet.
3.L1稀疏性的原因：在参数空间中，正方形比圆形更容易取到顶点。
4.稀疏性的好处：特征选择（参数变少，减小过拟合，减小维度灾难），可解释性强。当然，L2通过减小权重来减低模型复杂度，减小过拟合。
5.如何选择正则项：如果模型出现过拟合现象时，可以考虑正则化。当特征数量过多（需要特征选择）或者实际生产不能太多特征的时候需要L1正则。正则化还有其他方式，比如限制最大最小参数，比如CNN共享参数。
```
##三、Bagging和Boosting
```
1.Bagging模型是通过各自独立训练基模型，平均投票，易并行，这样容易降低方差。随机森林RF的话每棵树的数据是采用Bootstrap（有放回采样）方式抽取，每棵树随机使用一定量特征，引入了随机性，不剪枝也不易过拟合。因为有强大数定理，随着树的数量增加不易过拟合。对缺失值不敏感，对大量特征也适用（给出权重是通过三分之一的OOB的数据，引入随机噪声，观察误差的变化，变化越大，说明这个特征越重要。而GBDT可以采用Gini增益的变化来确定特征的重要性），缺点是当一个特征值的数量太多的时候可能造成这个特征权重过大。
2.Boosting是基于一个弱模型，通过加法，不断训练到误差减小的方向，不易并行，有利于降低偏差，对outlier敏感。Adaboost是通过不断增加误分类的数据的权重，且当前分类器的效果越好，进行投票的权重越大。梯度提升树是通过不断用新的基分类器去拟合残差（实际值-预测值）。GBDT（GBM，GBRT）是一种梯度提升树，使用CART回归树实现，用梯度去近似残差。而XGBoost是一种GBDT的高效实现，使用了二阶梯度来近似，而且基分类器不限于CART，引入了L1和L2正则，引入了学习速率shrinkage而不去拟合全部的残差来降低过拟合，在特征粒度而不是tree的粒度实现了并行（cache），同时也像随机森林一样采用了列采样，并且使用直方图算法（不必使用全部数据）加速分裂，对缺失值不再敏感。
3.RF比单棵决策树更优的理论依据，这里要假设每棵树的错误率相同并且每棵树相互独立，写出rf的错误率，然后用hoffeding不等式证明。
4.RF调参：n_estimators,n_job,max_depth
5.GBDT和RF区别
随机森林采用的是bagging的思想，bagging又称为bootstrap aggreagation，通过在训练样本集中进行有放回的采样得到多个采样集，基于每个采样集训练出一个基学习器，再将基学习器结合。随机森林在对决策树进行bagging的基础上，在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性的时候是在当前节点属性集合中选择最优属性，而随机森林则是对结点先随机选择包含k个属性的子集，再选择最有属性，k作为一个参数控制了随机性的引入程度。另外，GBDT训练是基于Boosting思想，每一迭代中根据错误更新样本权重，因此是串行生成的序列化方法，而随机森林是bagging的思想，因此是并行化方法。
```
##四、决策树的分裂指标
1.ID3
信息熵，
![](~/3.gif)
信息增益，
![](~/4.gif)

2.C4.5
因为有一些特征（比如id列）取值数量特别多，导致信息增益特别多，而实际上这些列意义不大，需要改成信息增益比，
![](~/5.gif)
3.CART
使用基尼系数（两次抽取到的分类不同的概率）来近似香农熵，
![](~/6.gif)
##五、SVM
1.SVM优点
(1)支持向量存储小，抗数据扰动（SVM的复杂度主要由支持向量刻画，而不是数据维度，不太容易过拟合）
(2)核引入非线性，相当于隐藏层
2.SVM推导
(1)目标：找到最大分离超平面，使结构风险最小
(2)线性可分下的硬间隔超平面，
![](~/7.gif)
(3)每个点到超平面的几何距离，
![](~/8.gif)
(4)最大化支持向量上的几何距离，
![](~/9.gif)
(5)因为取w，b取倍数无影响，不妨设1，等价有，
![](~/10.gif)
(6)不方便直接求，用拉格朗日对偶引入阿尔法，KKT条件，
![](~/11.gif)
![](~/12.gif)
(7)先求阿尔法，再求w，b。
3.若线性不可分。使用软间隔，增加C惩罚项。
4.非线性：核。
(0)线性核：简单，速度快，但是需要线性可分
(1)多项式核：比线性核拟合程度更强，知道具体的维度，但是高次容易出现数值不稳定，参数选择比较多。
(2)高斯核：拟合能力最强，但是要注意过拟合问题。不过只有一个参数需要调整。
(3)字符串核
(4)RBF径向核
(5)Sigmoid
5.损失函数是合页损失

##六、常用损失函数
1.平方损失（MSE，OLS，LSM）
![](~/13.gif)
2.0-1损失
![](~/14.gif)
3.对数损失(LR)
![](~/15.gif)
4.指数损失（Adaboost）
![](~/16.gif)
5.合页损失(HingeLoss,SVM)
![](~/17.gif)
6.交叉熵（Softmax，多分类）
![](~/18.gif)

##七、下降器
1.牛顿法和梯度下降的区别：梯度下降只利用一阶信息，牛顿法还利用了二阶梯度。一般牛顿法更快。

##八、过拟合
```
1.深度学习中如何处理过拟合问题：(1)参数正则化，L1,L2，最大最小限制 (2)增加数据 (3)提前终止 Early Stopping (4)CNN中的参数绑定，参数共享 (5)Dropout:随机删除一些神经元 (6)Batch Normalization，以mini-batch进行归一化。
2.欠拟合：一般来说欠拟合更容易解决一些，例如增加模型的复杂度，增加决策树中的分支，增加神经网络中的训练次数等等。
3.过拟合：一般认为过拟合是无法彻底避免的，因为机器学习面临的问题一般是np-hard,但是一个有效的解一定要在多项式内可以工作，所以会牺牲一些泛化能力。过拟合的解决方案一般有增加样本数量，对样本进行降维，降低模型复杂度，利用先验知识(L1,L2正则化)，利用cross-validation，early stopping等等。
4.决策树降低模型复杂度。预剪枝可以减少了大量生长的机会，是局部优化，比较快。后剪枝比较慢，但是比较全局。
```

##九、聚类
```
1.列举三个聚类算法：k-means;DBSCAN密度聚类；CLIQUE网格聚类。
2.用过哪些聚类算法，解释密度聚类算法。
k-means算法，聚类性能的度量一般分为两类，一类是聚类结果与某个参考模型比较(外部指标)，另外是直接考察聚类结果(内部指标)。后者通常有DB指数和DI，DB指数是对每个类，找出类内平均距离/类间中心距离最大的类，然后计算上述值，并对所有的类求和，越小越好。类似k-means的算法仅在类中数据构成簇的情况下表现较好，密度聚类算法从样本密度的角度考察样本之间的可连接性，并基于可连接样本不断扩展聚类蔟得到最终结果。DBSCAN(density-based spatial clustering of applications with noise)是一种著名的密度聚类算法，基于一组邻域参数进行刻画，包括邻域，核心对象(邻域内至少包含个对象)，密度直达(j由i密度直达，表示j在i的邻域内，且i是一个核心对象)，密度可达(j由i密度可达，存在样本序列使得每一对都密度直达)，密度相连(xi,xj存在k,i,j均有k可达)，先找出样本中所有的核心对象，然后以任一核心对象作为出发点，找出由其密度可达的样本生成聚类蔟，直到所有核心对象被访问过为止。
3.聚类算法中的距离度量一般用闽科夫斯基距离，在p取不同的值下对应不同的距离，例如p=1的时候对应曼哈顿距离，p=2的情况下对应欧式距离，p=inf的情况下变为切比雪夫距离，还有jaccard距离，幂距离(闽科夫斯基的更一般形式),余弦相似度，加权的距离，马氏距离(类似加权)作为距离度量需要满足非负性，同一性，对称性和直递性，闽科夫斯基在p>=1的时候满足读来那个性质，对于一些离散属性例如{飞机，火车，轮船}则不能直接在属性值上计算距离，这些称为无序属性，可以用VDM(Value Diffrence Metrix)，属性u上两个离散值a,b之间的VDM距离定义。为其中表示在第i个簇中属性u上a的样本数，样本空间中不同属性的重要性不同的时候可以采用加权距离，一般如果认为所有属性重要性相同则要对特征进行归一化。一般来说距离需要的是相似性度量，距离越大，相似度越小，用于相似性度量的距离未必一定要满足距离度量的所有性质，例如直递性。比如人马和人，人马和马的距离较近，然后人和马的距离可能就很远。
```

##十、特征选择
```
1.过滤式：先选特征再训练，计算特征与响应变量的相关关系，比如使用皮尔逊系数（线性），比如互信息。
2.包裹式：直接把训练结果作为特征子集评价标准。
3.嵌入式：训练的过程就进行了特征选择，比如Lasso，RF，XGBoost，深度学习自动选择特征。
4.为什么要进行特征选择？
首先在现实任务中我们会遇到维数灾难的问题(样本密度非常稀疏)，若能从中选择一部分特征，那么这个问题能大大缓解，另外就是去除不相关特征会降低学习任务的难度，增加模型的泛化能力。
```

##十一、深度学习
```
1.梯度消失（弥散）：根据BP算法的链式法则偏导向上层传递，每次乘积小于1，到前面几层就调不动了。
2.梯度爆炸：大于1，前面的梯度抖动就很大。
3.解决：使用ReLU单元，使用Batch-Normalization,使用Dropout。逐层训练。
```

##十二、EM算法有哪些
1.有k-means,gmm（混合高斯模型）,hmm的求解

##十三、回归树和决策树的区别
1.决策树用于回归是回归树，用于分类是分类树。回归，y值均方差作为划分依据，比如划分成两堆，均方差加权。
2.连续值和缺失值的处理，对于连续属性a,将a在D上出现的不同的取值进行排序，基于划分点t将D分为两个子集。一般对每一个连续的两个取值的中点作为划分点，然后根据信息增益选择最大的。与离散属性不同，若当前节点划分属性为连续属性，该属性还可以作为其后代的划分属性。

##十四、LR、RF、GBDT分析优缺点
1.LR适合于广义线性的二分类，模型简单。但是容易欠拟合，不能多分类，不能回归，不能非线性。
2.RF效果一般比较好，对高维特征比较方便，能给出特征权重，容易并行，能够处理缺失值数据。但是当一个特征取值较多时，这个特征的权重会过大。
3.GBDT的效果也很好，传统的GBDT是串行的。XGBoost的优点就是他的缺点。

##十五、推荐系统冷启动问题
1.利用用户注册信息冷启动
2.利用物品内容信息冷启动
3.利用用户对特定物品的反馈
4.发挥专家作用

##十六、有监督和无监督的区别
1.监督学习建立X->y的关系。无监督没有y，根据X研究数据规律。

##十七、多分类问题
一般将二分类推广到多分类的方式有三种，一对一，一对多，多对多。
1.一对一：将N个类别两两配对，产生N(N-1)/2个二分类任务，测试阶段新样本同时交给所有的分类器，最终结果通过投票产生。
2.一对多：每一次将一个例作为正例，其他的作为反例，训练N个分类器，测试时如果只有一个分类器预测为正类，则对应类别为最终结果，如果有多个，则一般选择置信度最大的。从分类器角度一对一更多，但是每一次都只用了2个类别，因此当类别数很多的时候一对一开销通常更小(只要训练复杂度高于O(N)即可得到此结果)。
3.多对多：若干各类作为正类，若干个类作为反类。注意正反类必须特殊的设计。

##十八、SVM和LR
SVM既可以用于分类问题，也可以用于回归问题，并且可以通过核函数快速的计算，LR实现简单，训练速度非常快，但是模型较为简单，决策树容易过拟合，需要进行剪枝等。从优化函数上看，soft margin的SVM用的是hinge loss,而带L2正则化的LR对应的是cross entropy loss，另外adaboost对应的是exponential loss。所以LR对远点敏感，但是SVM对outlier不太敏感，因为只关心support vector，SVM可以将特征映射到无穷维空间，但是LR不可以，一般小数据中SVM比LR更优一点，但是LR可以预测概率，而SVM不可以，SVM依赖于数据测度，需要先做归一化，LR一般不需要，对于大量的数据LR使用更加广泛，LR向多分类的扩展更加直接，对于类别不平衡SVM一般用权重解决，即目标函数中对正负样本代价函数不同，LR可以用一般的方法，也可以直接对最后结果调整(通过阈值)，一般小数据下样本维度比较高的时候SVM效果要更优一些。

##十九、凸不凸
如果函数有二阶导数，那么如果函数二阶导数为正，或者对于多元函数，Hessian矩阵半正定则为凸函数。


##二十、如何解决类别不平衡问题
通常有三种做法，一种是对训练集的负样本进行欠采样，第二种是对正例进行升采样，第三种是直接基于原始训练集进行学习，在预测的时候再改变阈值，称为阈值移动。注意过采样一般通过对训练集的正例进行插值产生额外的正例，而欠采样将反例划分为不同的集合供不同的学习器使用。

##二十一、什么是偏差与方差
1.泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。
2.从目标函数看：损失函数减低偏差，正则项减低方差。
3.从集成模型角度看：bagging降低方差，boosting降低偏差。

##二十二、神经网络的原理，如何进行训练
神经网络自发展以来已经是一个非常庞大的学科，一般而言认为神经网络是由单个的神经元和不同神经元之间的连接构成，不够的结构构成不同的神经网络。最常见的神经网络一般称为多层前馈神经网络，除了输入和输出层，中间隐藏层的个数被称为神经网络的层数。BP算法是训练神经网络中最著名的算法，其本质是梯度下降和链式法则。

##二十三、EM
1.采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。
2.k-means算法是高斯混合聚类在混合成分方差相等，且每个样本仅指派一个混合成分时候的特例。注意k-means在运行之前需要进行归一化处理，不然可能会因为样本在某些维度上过大导致距离计算失效。k-means中每个样本所属的类就可以看成是一个隐变量，在E步中，我们固定每个类的中心，通过对每一个样本选择最近的类优化目标函数，在M步，重新更新每个类的中心点，该步骤可以通过对目标函数求导实现，最终可得新的类中心就是类中样本的均值。k-means对初始值和k值的选择敏感。

##二十三、解释贝叶斯公式和朴素贝叶斯分类
1.最小化分类错误的贝叶斯最优分类器等价于最大化后验概率。
基于贝叶斯公式来估计后验概率的主要困难在于，条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计得到。朴素贝叶斯分类器采用了属性条件独立性假设，对于已知的类别，假设所有属性相互独立。这样，朴素贝叶斯分类则定义为。如果有足够多的独立同分布样本，那么可以根据每个类中的样本数量直接估计出来。在离散情况下先验概率可以利用样本数量估计或者离散情况下根据假设的概率密度函数进行最大似然估计。朴素贝叶斯可以用于同时包含连续变量和离散变量的情况。如果直接基于出现的次数进行估计，会出现一项为0而乘积为0的情况，所以一般会用一些平滑的方法，例如拉普拉斯修正.
2.贝叶斯：后验=先验x似然。
3.生成模型（朴素贝叶斯、HMM、GMM、LDA）：需要先求一个联合概率密度。

##二十四、TF-IDF是什么
　　TF指Term frequecy,代表词频,IDF代表inverse document frequency,叫做逆文档频率，这个算法可以用来提取文档的关键词，首先一般认为在文章中出现次数较多的词是关键词，词频就代表了这一项，然而有些词是停用词，例如的，是，有这种大量出现的词，首先需要进行过滤，比如过滤之后再统计词频出现了中国，蜜蜂，养殖且三个词的词频几乎一致，但是中国这个词出现在其他文章的概率比其他两个词要高不少，因此我们应该认为后两个词更能表现文章的主题，IDF就代表了这样的信息，计算该值需要一个语料库，如果一个词在语料库中出现的概率越小，那么该词的IDF应该越大，一般来说TF计算公式为(某个词在文章中出现次数/文章的总词数)，这样消除长文章中词出现次数多的影响，IDF计算公式为log(语料库文章总数/(包含该词的文章数)+1)。将两者乘乘起来就得到了词的TF-IDF。传统的TF-IDF对词出现的位置没有进行考虑，可以针对不同位置赋予不同的权重进行修正，注意这些修正之所以是有效的，正是因为人观测过了大量的信息，因此建议了一个先验估计，人将这个先验估计融合到了算法里面，所以使算法更加的有效。

##二十五、如何选择模型
1.考虑数据特点，考虑各模型优缺点
2.使用交叉验证方式，面向性能目标

##二十六、为什么激活函数要用ReLU
1.优点(1)因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息。减小了梯度弥散。(2)有大量输出为0，造成稀疏性，减小过拟合。(3)比sigmoid计算简单多了。
2.缺点是不能用Gradient-Based方法。同时如果de-active了，容易无法再次active。不过有办法解决，使用maxout激活函数。
3.Leaky ReLU